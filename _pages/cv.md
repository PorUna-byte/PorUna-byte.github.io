---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

# Jiayi Fu
**Location:** Shanghai, China \
**Phone:** +86 19370551152 \
**Email:** [fujy22@m.fudan.edu.cn](mailto:fujy22@m.fudan.edu.cn) \
**GitHub:** [github.com/PorUna-byte](https://github.com/PorUna-byte) \
**Homepage:** [https://poruna-byte.github.io/](https://poruna-byte.github.io/)\
**Google Scholar:** [https://scholar.google.com/citations?hl=en&user=V2QlLjkAAAAJ](https://scholar.google.com/citations?hl=en&user=V2QlLjkAAAAJ)

## Education
**Fudan University** \
*Master's Student in Natural Language Processing* \
Expected Graduation: June 2025

**Harbin Institute of Technology** \
*Bachelor of Science in Computer Science and Technology* \
Graduated: June 2022

## Internship Experience
Currently, I have no formal internship experience. However, I have engaged extensively in academic projects and collaborative coding initiatives that have honed my skills and understanding in the field.

## Projects
**Travel Agent** \
Primary developer of an intelligent travel planner leveraging ChatGPT. Focused on addressing the challenges of creating travel plans that are reasonable, personalized, and up-to-date. Developed strategies to enhance the reasoning capabilities of base models and integrate user preferences with current global information.

**RLHF: Reinforcement Learning From Human Feedback** \
I am engaged into a project that tries to reproduce the RLHF technique based on Llama2. In this project, my role is to train a reward model. Basically, the reward model is trained via a contrastive loss and a language modeling loss. The reward model is initialized as original Llama2-7B, and substitute the final layer to output a scalar as reward. The final reward for a prompt and a completion is given by the reward model and a KL-divergence between policy model and initial supervised model.   

## Acadamic Research
**GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-Trick** \
I am the First author of a paper aimed at embedding highly detectable watermarks while maintaining text quality and diversity into machine generated texts. Our paper is based on Aarason's Exponential watermark and explores several ways to enhance the generation diversity of his exponential watermark.

**P4: Plug-and-Play Discrete Prompting for Large Language Models Personalization** \
In this paper, we propose a plug-and-play prompting method to manipulate the LLMs' personality traits. Specifically, we append discrete personalized suffixes, automatically generated through an aggregated gradient-based search method, to the user query or dialog histories and induce LLMs to respond with target personalities.

## Skills
- **Programming Languages:** Python, Java, C/C++, Go
- **Technologies:** Familiar with foundational software principles. Contributed to various open-source projects linked to coursework, such as MIT 6.828 (Operating System), MIT 6.824 (Distributed System), CMU Bustub (Database Systems), CMU needle (DeepLearning System) and Stanford CS144 (Computer Network), etc...
- **Soft Skills:** Demonstrated leadership, effective communication, and problem-solving capabilities.

## Extracurricular Activities
**Fudan Football Team** \
*Position: Left Forward, March 2024 â€“ Present* \
Key striker, contributing significantly to team dynamics and success.